#####################################################################################
# General configuration file to configure RL-Studio in the training or inference mode
#####################################################################################

#####################################################################################
# settings: General parameters
#
# Most relevant params:
#   model_state_name: agent name
#   total_episodes: training epochs
#   training_time: in hours
#   save_episodes: variable for TensorFlow savings
#####################################################################################

settings:
  output_dir: "./logs/"
  save_model: True
  save_positions: True
  debug_level: DEBUG
  telemetry: False
  telemetry_mask: True
  plotter_graphic: False
  model_state_name: f1_renault_multicamera_multilaser #f1_renault
  total_episodes: 50_000
  training_time: 6
  save_episodes: 50
  save_every_step: 100
  ros_master_uri: '11311'
  gazebo_master_uri: '11345'  

#####################################################################################
# agent: every agent configures states, rewards and sensors
#
# Most relevant params:
#   image_resizing: percentage of image redimension to feed neural nets. I.e. 10 means a width of 64 pixels and height of 48 pixels
#   num_regions: in simplified perception, number of image vertical divisions in which every state falls
#   new_image_size: every image size is fixed in to feed neural net. I.e. 32 means a size of 32x32 pixels
#   state_space: configurates how input data is feeding. Image means raw data from camera sensor. sp1,...spn means simplified perception of 1 to n points. 
#   image: 0: distance from image midline down in pixels
#   sp1, sp3, sp5, spn: simplified perception with 1, 3, 5 or n points respectively. Every number represents pixels from image midline down
#   reward_function: discrete_follow_line represents a hardcoded reward function in follow line project, linear_follow_line means regression function in follow line project
#
#####################################################################################

agent:
  f1:
    agent_name: f1
    camera_params:
      width: 640
      height: 480
      center_image: 320
      raw_image: False
      image_resizing: 100
      new_image_size: 32
      num_regions: 16
      lower_limit: 239
    states:
      state_space: sp3 #sp1
      image:
        0: [3]
      sp1: 
        0: [30]
      sp3:
        0: [60, 120, 180]
      sp5: 
        0: [30, 80, 130, 190, 239]  
      spn: 
        0: [10]   
    rewards:
      reward_function: discrete_follow_right_lane #linear_follow_line
      discrete_follow_line:
        from_0_to_02: 10
        from_02_to_04: 2
        from_others: 1
        penal: -100
        min_reward: 1_000
        highest_reward: 100
      linear_follow_line:
        beta_0: 3
        beta_1: -0.1
        penal: 0
        min_reward: 1_000
        highest_reward: 100
      discrete_follow_right_lane:
        from_10: 10
        from_02: 2
        from_01: 1
        penal: -100
        min_reward: 5_000
        highest_reward: 100
#####################################################################################
# actions: mainly divided into continuous and discrete sets of actions. In continuous for plannar agents it is divided in min and max.
# In other cases, we create a set of actions, 3, 5 or more, where every one is [linear velocity, angular velocity]
#
#   actions_number: for plannar agents, two actions are executed
#   simple: 
#         0: [3 m/sec, 0 rad/sec]
#         1: [2 m/sec, 1 rad/sec]
#         2: [2 m/sec, -1 rad/sec]
#
#####################################################################################

actions:
    actions_number: 3 # simple:3, medium:5, continuous: 2
    actions_set: simple #simple
    available_actions:
      simple:
        0: [ 3, 0 ]
        1: [ 2, 1 ]
        2: [ 2, -1 ]
      medium:
        0: [ 3, 0 ]
        1: [ 2, 1 ]
        2: [ 2, -1 ]
        3: [ 1, 1.5 ]
        4: [ 1, -1.5 ]
      hard:
        0: [ 3, 0 ]
        1: [ 2, 1 ]
        2: [ 2, -1 ]
        3: [ 1.5, 1 ]
        4: [ 1.5, -1 ]
        5: [ 1, -1.5 ]
        6: [ 1, -1.5 ]
      test:
        0: [ 0, 0 ]
      continuous:
        v_min: 2
        v_max: 10
        w_right: -2
        w_left: 2


#####################################################################################
# environments: configurates every param in all envs. 
#
# Most relevant params:
#   env_name: F1Env-v0, RobotMeshEnv-v0, myCartpole-v0, MyMountainCarEnv-v0
#   training_type: qlearn_camera, qlearn_laser, dqn, manual, ddpg
#   circuit_positions_set: different positions in Gazebo simulator for every environment. Set represents x, y, z, 0, roll, pitch, yaw          
#   start_pose: agent initial pose in every training. It takes number from circuit_positions_set param
#   alternate_pose: if True, the agent randoms initial pose, taking from circuit_positions_set param. Otherwise, it takes start_pose number

#####################################################################################

environments:
  simple:
    env_name: F1Env-v0
    circuit_name: simple
    training_type: qlearn_camera_follow_lane #qlearn_camera_follow_line, qlearn_laser_follow_line, qlearn_camera_follow_lane, ddpg_follow_line, ddpg_follow_lane, dqn_follow_line, dqn_follow_lane, 
    launchfile: simple_circuit_no_wall.launch
    environment_folder: f1
    robot_name: f1_renault_multicamera_multilaser # f1_renault
    start_pose: 0
    alternate_pose: True
    estimated_steps: 20_000
    sensor: camera
    circuit_positions_set:
      0: [52.800, -39.486, 0.004, 0, 0, 1.57, -1.57] # near to first curve
      #0: [52.800, -8.734, 0.004, 0, 0, 1.57, -1.57] # Finish line
      #0: [53.462, -41.988, 0.004, 0, 0, 1.57, -1.57]
      1: [52.97, -42.06, 0.004, 0, 0, 1.57, -1.57]
      #1: [53.462, -8.734, 0.004, 0, 0, 1.57, -1.57]
      #2: [39.712, -30.741, 0.004, 0, 0, 1.56, 1.56]
      2: [40.2, -30.741, 0.004, 0, 0, 1.56, 1.56]
      #3: [-6.861, -36.481, 0.004, 0, 0.01, -0.858, 0.613]
      3: [0, 31.15, 0.004, 0, 0.01, 0, 0.31]
      #4: [20.043, 37.130, 0.003, 0, 0.103, -1.4383, -1.4383]
      4: [19.25, 43.50, 0.004, 0, 0.0, 1.57, -1.69]
  nurburgring:
    env_name: F1Env-v0
    circuit_name: nurburgring
    training_type: qlearn_camera
    launchfile: nurburgring_line.launch
    environment_folder: f1
    robot_name: f1_renault
    start_pose: 0
    alternate_pose: True
    estimated_steps: 3500
    sensor: camera
    circuit_positions_set:
      0: [-32.3188, 12.2921, 0, 0.0014, 0.0049, -0.2727, 0.9620]
      1: [-30.6566, -21.4929, 0, 0.0014, 0.0049, -0.4727, 0.8720]
      2: [28.0352, -17.7923, 0, 0.0001, 0.0051, -0.028, 1]
      3: [88.7408, -31.7120, 0, 0.0030, 0.0041, -0.1683, 0.98]
      4: [-73.2172, 11.8508, 0, 0.0043, -0.0027, 0.8517, 0.5173]
      5: [-73.6672, 37.4308, 0, 0.0043, -0.0027, 0.8517, 0.5173]
  montreal:
    env_name: F1Env-v0
    circuit_name: montreal
    training_type: qlearn_camera
    launchfile: montreal_line.launch
    environment_folder: f1
    robot_name: f1_renault
    start_pose: 0
    alternate_pose: True
    estimated_steps: 8000
    sensor: camera
    circuit_positions_set:
      0: [-201.88, -91.02, 0, 0.00, 0.001, 0.98, -0.15]
      1: [-278.71, -95.50, 0, 0.00, 0.001, 1, 0.03]
      2: [-272.93, -17.70, 0, 0.0001, 0.001, 0.48, 0.87]
      3: [-132.73, 55.82, 0, 0.0030, 0.0041, -0.02, 0.9991]
      4: [294.99, 91.54, 0, 0.0043, -0.0027, 0.14, 0.99]
  curves:
    env_name: F1Env-v0
    circuit_name: curves
    training_type: qlearn_camera
    launchfile: many_curves.launch
    environment_folder: f1
    robot_name: f1_renault
    start_pose: 0
    alternate_pose: False
    estimated_steps: 4000
    sensor: camera
  simple_laser:
    env_name: F1Env-v0
    circuit_name: simple_laser
    training_type: qlearn_laser
    launchfile: f1_montreal.launch
    environment_folder: f1
    robot_name: f1_renault
    start_pose: 0
    alternate_pose: False
    estimated_steps: 4000
    sensor: laser
  manual:
    env_name: F1Env-v0
    circuit_name: manual
    training_type: qlearn_camera
    launchfile: simple_circuit.launch
    environment_folder: f1
    robot_name: f1_renault
    start_pose: 0
    alternate_pose: False
    estimated_steps: 4000
    sensor: camera

#####################################################################################
# inference: loading training files

#####################################################################################
inference:
  qlearn:
    inference_file: checkpoints/follow_lane/qlearn_models/1_20221128_1048_Circuit-simple_States-sp3_Actions-simple_epsilon-0.9_epoch-5_step-201_reward-2010_QTABLE.pkl
    actions_file: checkpoints/follow_line/qlearn_models/actions_set_20221128_0938

#####################################################################################
# algorithm: every particular param

#####################################################################################
algorithm:
    qlearn:
      alpha: 0.2
      epsilon: 0.90
      epsilon_min: 0.05      
      gamma: 0.9
    dqn:
      alpha: 0.8
      gamma: 0.9
      epsilon: 0.99
      epsilon_discount: 0.9986
      epsilon_min: 0.05
      model_name: DQN_sp_16x16
      replay_memory_size: 50_000
      min_replay_memory_size: 1000
      minibatch_size: 64
      update_target_every: 5
      memory_fraction: 0.20
      buffer_capacity: 100_000
      batch_size: 64
    sarsa:
    ddpg:
      gamma: 0.9
      tau: 0.005
      std_dev: 0.2
      model_name: DDPG_Actor_conv2d32x64_Critic_conv2d32x64
      replay_memory_size: 50_000
      memory_fraction: 0.20
      critic_lr: 0.002
      actor_lr: 0.001
      buffer_capacity: 100_000
      batch_size: 64
